{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import \n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "# import nltk\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec\n",
    "# import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the GLOVE model\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-trained word2vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# Loading pretrained glove model\n",
    "glove = loadGloveModel(\"../../glove.6B/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is taken from  https://github.com/marcotcr/lime-experiments/blob/master/load_datasets.py and modified a bit\n",
    "def LoadMultiDomainDataset(path_data, remove_bigrams=True):\n",
    "    random.seed(1)\n",
    "    pos = []\n",
    "    neg = []\n",
    "    def get_words(line, remove_bigrams=True):\n",
    "        z = [tuple(x.split(':')) for x in re.findall('\\w*?:\\d', line)]\n",
    "        if remove_bigrams:\n",
    "            z = ' '.join([' '.join([x[0]] * int(x[1])) for x in z if '_' not in x[0]])\n",
    "        else:\n",
    "            z = ' '.join([' '.join([x[0]] * int(x[1])) for x in z])\n",
    "        return z\n",
    "    for line in open(os.path.join(path_data, 'books_negative.review')):\n",
    "        neg.append(get_words(line, remove_bigrams))\n",
    "    for line in open(os.path.join(path_data, 'books_positive.review')):\n",
    "        pos.append(get_words(line, remove_bigrams))\n",
    "    data = pos+neg\n",
    "    labels = [1] * len(pos) + [0]* len(neg)\n",
    "    return data, np.array(labels).reshape(len(labels),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model(df):\n",
    "    # seleting X and y and splitting data into train and test datasets\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = books_df.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Grid search for best parameters\n",
    "    rfc = RandomForestClassifier(random_state=42) \n",
    "    param_grid = {\n",
    "        'n_estimators': [1000],\n",
    "        'max_depth':[6,8],\n",
    "        'max_features': ['auto']\n",
    "    }\n",
    "    CV = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5, verbose=3)\n",
    "    \n",
    "    # fitting to trian data and calculating training accuarcy\n",
    "    CV.fit(X_train, y_train)   \n",
    "    print('best train accuracy: {:.2f}% & best parameter combination: {}'.format(CV.best_score_*100,CV.best_params_))\n",
    "    \n",
    "    # Calculating test accuracy\n",
    "    y_pred = CV.best_estimator_.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print('test accuracy: {:.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "books_reviews, books_targets = LoadMultiDomainDataset('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mean word2vec embedding for each review\n",
    "def mean_word2vec_vector(reviews):\n",
    "    mean_vecs=[]\n",
    "    for review in reviews:\n",
    "        vecs=[]\n",
    "        tokens = review.split()\n",
    "        for token in tokens:\n",
    "            if token in word2vec:\n",
    "                vec = word2vec.word_vec(token)\n",
    "                vecs.append(vec)\n",
    "            mean_vec = np.mean(vecs,axis=0)\n",
    "        mean_vecs.append(mean_vec)\n",
    "    return np.array(mean_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saibabu.udayagiri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\saibabu.udayagiri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# creating dataframe\n",
    "books_vecs = mean_word2vec_vector(books_reviews)\n",
    "books_arr = np.hstack((books_vecs,books_targets))\n",
    "books_df = pd.DataFrame(books_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.775, total=  11.4s\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.831, total=  12.4s\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   23.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.812, total=  14.3s\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.787, total=  15.3s\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.766, total=  17.0s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.787, total=  13.0s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.822, total=  12.1s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.797, total=  11.6s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.794, total=  13.9s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.784, total=  13.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best train accuracy: 79.69% & best parameter combination: {'max_depth': 8, 'max_features': 'auto', 'n_estimators': 1000}\n",
      "test accuracy: 79.75%\n"
     ]
    }
   ],
   "source": [
    "# Building RF model\n",
    "rf_model(books_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification using GLove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mean glove embeddings for each review\n",
    "def mean_glove_vector(reviews):\n",
    "    mean_vecs=[]\n",
    "    for review in reviews:\n",
    "        vecs=[]\n",
    "        tokens = review.split()\n",
    "        for token in tokens:\n",
    "            if token in glove.keys():\n",
    "                vec = glove[token]\n",
    "                vecs.append(vec)\n",
    "            mean_vec = np.mean(vecs,axis=0)\n",
    "        mean_vecs.append(mean_vec)\n",
    "    return np.array(mean_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saibabu.udayagiri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\saibabu.udayagiri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.775, total=   8.9s\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.806, total=   9.8s\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   18.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.787, total=  10.4s\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.756, total=   9.1s\n",
      "[CV] max_depth=6, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=6, max_features=auto, n_estimators=1000, score=0.762, total=  10.3s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.766, total=  12.1s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.800, total=  13.2s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.781, total=  14.0s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.772, total=  12.8s\n",
      "[CV] max_depth=8, max_features=auto, n_estimators=1000 ...............\n",
      "[CV]  max_depth=8, max_features=auto, n_estimators=1000, score=0.775, total=  12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best train accuracy: 77.88% & best parameter combination: {'max_depth': 8, 'max_features': 'auto', 'n_estimators': 1000}\n",
      "test accuracy: 77.00%\n"
     ]
    }
   ],
   "source": [
    "# creating dataframe\n",
    "books_vecs = mean_glove_vector(books_reviews)\n",
    "books_arr = np.hstack((books_vecs,books_targets))\n",
    "books_df = pd.DataFrame(books_arr)\n",
    "\n",
    "# Building RF model\n",
    "rf_model(books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
